울회사 인터넷pc 코드 옮기기용임


"Multiple Imputation for Nonresponse in Surveys" by Donald B. Rubin: 이 책에서 Rubin은 단순히 평균값 등으로 대체하는 방법보다 다중 대체법의 우월성을 강조합니다. 이는 상당히 높은 비율의 결손값(예를 들어 20~40%)에 대해서도 유효



from missingpy import MissForest
import numpy as np
from scipy.stats import chi2

def littles_mcar_test(data):
    data = data.copy()
    data['nullity'] = data.isnull().apply(sum, axis=1)
    
    cols = data.columns.tolist()
    cols.remove('nullity')
    
    observed = []
    expected = []
    
    for col in cols:
        observed += [data.loc[data[col].isna(), 'nullity'].tolist()]
        expected += [data.loc[data[col].notna(), 'nullity'].tolist()]
        
    observed = np.concatenate(observed)
    expected = np.concatenate(expected)
    
    chi_square_statistic = ((observed - expected)**2 / expected).sum()
    
    df = (data.shape[1] - 1) * (data['nullity'].nunique() - 1)
    
    p_value = 1 - chi2.cdf(chi_square_statistic, df)

return p_value

# 사용 예시:
p_value = littles_mcar_test(your_dataframe)








import fileinput
import sys

filename = "/usr/local/lib/python3.x/dist-packages/missingpy/utils.py"
# 위 경로는 실제 missingpy 패키지가 설치된 위치에 따라 다를 수 있습니다.
# 적절한 경로로 변경해주세요.

with fileinput.FileInput(filename, inplace=True) as file:
    for line in file:
        print(line.replace("from sklearn.neighbors.base import NeighborsBase, KNeighborsMixin",
                            "from sklearn.neighbors import NeighborsBase, KNeighborsMixin"), end='')







import pandas as pd
import numpy as np

def test_mcar(df):
    # 결측 여부를 나타내는 새로운 데이터 프레임 생성
    df_nan = df.isnull().astype(int)
    
    # 모든 변수들 사이의 상관 계수 계산
    corr_matrix = df_nan.corr().abs()

    # 대각선 요소 제거
    for x in range(len(corr_matrix )):
        corr_matrix.iloc[x,x] = 0

    # 상관 계수가 유의미하게 큰 경우가 있는지 확인
    return corr_matrix.max().max()

# 사용 예시:
max_corr = test_mcar(your_dataframe)
if max_corr < 0.05:  # 임곗값은 실험적으로 설정해야 합니다.
    print("Data is likely MCAR")
else:
    print("Data is likely MAR or NMAR")






import pandas as pd
import numpy as np

def test_mcar(df):
    # 각 컬럼별로 결측 여부를 나타내는 새로운 데이터 프레임 생성
    df_nan = df.isnull().astype(int)
    
    results = {}
    
    # 각 컬럼별로 다른 모든 변수들과의 상관 계수 계산
    for col in df.columns:
        corr_matrix = df_nan[[c for c in df.columns if c != col]].corrwith(df_nan[col]).abs()
        
        # 상관 계수가 유의미하게 큰 경우가 있는지 확인
        max_corr = corr_matrix.max()
        
        results[col] = max_corr < 0.05  # 임곗값은 실험적으로 설정해야 합니다.
    
    return results

# 사용 예시:
results = test_mcar(your_dataframe)

for column, is_mcar in results.items():
    if is_mcar:
        print(f"{column} is likely MCAR")
    else:
        print(f"{column} is likely MAR or NMAR")






missing_cols = df_adj_mv.columns[df_adj_mv.isnull().any()].tolist()
df_with_missing_values = df_adj_mv[missing_cols]




-----------------------

def fill_categorical(data, numeric_columns, categorical_columns, target_col_val='CODE_GENDER' ,epochs_val=30, batch_size_val=32 ):
    
    # 결측치 있는 데이터 준비
    data_with_missing = data.copy()

    # 결측치 채우기를 위한 데이터 준비
    target_column = target_col_val  # 채울 결측치가 있는 컬럼명
    input_numeric_columns = numeric_columns[1:]

    # 성별 전용
    data_with_missing.loc[data_with_missing['CODE_GENDER'] == 'XNA', 'CODE_GENDER'] = np.nan

    # 결측치가 없는 데이터 추출 추후 행 인덱스
    no_missing_data = data_with_missing[data_with_missing[target_column].notnull()]
    # 결측치 추후에 어떻게 채워지는 지 확인하기 위한 인덱스
    missing_data_index = data_with_missing[target_column].isnull()



    # 범주형 변수 인코딩
    encoder = OneHotEncoder(drop='first')
    categorical_data_encoded = encoder.fit_transform(data_with_missing[categorical_columns]).toarray()

    # 모든 입력 데이터 결합
    input_data_encoded = np.hstack((data_with_missing[input_numeric_columns].values, categorical_data_encoded))
    len(input_data_encoded)

    # 데이터 스케일링
    scaler = StandardScaler()
    input_data_scaled = scaler.fit_transform(input_data_encoded)


    print(len(input_data_scaled))

    train_data_scaled = input_data_scaled[data_with_missing[target_column].notnull()]

    print(len(train_data_scaled))

    # 결측치를 채울 Autoencoder 모델 구성
    input_dim = input_data_scaled.shape[1]



    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(input_dim,)),
        tf.keras.layers.Dense(65, activation='relu'),
        tf.keras.layers.Dropout(0.5),  # Dropout 추가
        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        tf.keras.layers.Dropout(0.5),  # Dropout 추가
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(2, activation='softmax')  # 선형 활성화 함수 사용
    ])

    model.compile(optimizer='rmsprop',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])


    # 타겟 컬럼 추후 원래 레이블으로 표기하기 위함

    # LabelEncoder 객체 생성 및 학습
    encoder_target = LabelEncoder()
    encoder_target.fit(no_missing_data[target_column])
    integer_encoded_labels = encoder_target.transform(no_missing_data[target_column])
    integer_encoded_labels



    one_hot_train_labels = to_categorical(integer_encoded_labels)

    # 모델 훈련전에 검증 데이터 셋으로 분류해 모델 학습이 제대로 되는지 평가

    X_train, X_val, y_train, y_val = train_test_split(train_data_scaled, one_hot_train_labels, test_size=0.2, random_state=42)

    # 과적합 방지 목적으로, 더이상 검증 데이터의 정확성 안올라가면 학습 중지
    early_stopping_cb = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    # 모델 훈련
    model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs=epochs_val, batch_size=batch_size_val, callbacks=[early_stopping_cb])
    

    
    missing_data = input_data_scaled[data_with_missing[target_column].isnull()]
    print(len(missing_data))
    print(len(missing_data[0]))

    # 훈련된 모델로 결측치 예측 및 채우기
    predicted_values = model.predict(missing_data)
    filled_values = predicted_values.squeeze()


    original_label=np.argmax(filled_values, axis=1)
    predicted_labels_string = encoder_target.inverse_transform(original_label)

    # 결측치를 채워 넣음
    data_with_fill = data_with_missing.copy()
    data_with_fill[target_column+"_predicted_flag"] = "N"
    data_with_fill.loc[data_with_missing[target_column].isnull(), target_column+"_predicted_flag"] = "Y"
    
    data_with_fill.loc[data_with_missing[target_column].isnull(), target_column] = predicted_labels_string


    print(data_with_fill.loc[missing_data_index, target_column].head())
    return data_with_fill
--------------------------------------



-----------------------------------------
# 예측변수 다음예측에 사용
numeric_columns,categorical_columns,missing_values = columns_classification(data)
FILLED_DATA =fill_categorical(data, numeric_columns, categorical_columns, target_col_val='CODE_GENDER' ,epochs_val=30, batch_size_val=32 )
numeric_columns,categorical_columns,missing_values = columns_classification(FILLED_DATA)
FILLED_DATA =fill_categorical(FILLED_DATA, numeric_columns, categorical_columns, target_col_val='FLAG_OWN_REALTY' ,epochs_val=30, batch_size_val=32 )

for i in range(len(missing_values)-1):
    numeric_columns,categorical_columns,missing_values = columns_classification(FILLED_DATA)
    FILLED_DATA =fill_numeric(FILLED_DATA, numeric_columns, categorical_columns, target_col_val=missing_values[0] ,epochs_val=32, batch_size_val=32 )
----------------------------------------

MISSING VALUE에 속하는 ORGANIZATION TYPE 관련해서
ValueError: could not convert string to float: 'Business Entity Type 3'

이러한 에러가 떠 인코딩 문제인거야>


-----------------------------

def plot_distribution(df, feature, target, color):
    
    plt.figure(figsize=(10,6))
    plt.title("Distribution of %s by Target" % feature)
    
    # For each value of the target variable, plot the distribution of the feature
    for target_value in df[target].unique():
        sns.distplot(df[df[target] == target_value][feature].dropna(),
                     color=color[target_value],
                     kde=True,
                     bins=100,
                     label=f'Target {target_value}')
        
    # Disable scientific notation 
    plt.ticklabel_format(style='plain', axis='y')
    plt.ticklabel_format(style='plain', axis='x')
    
    
    plt.legend()
    plt.show();

이러한 분포함수를 구성했는데 박스플롯도 이렇게 타겟여부 고려해서 그릴수 있게 이와같은 유형으로 파이썬 코드 함수짜줘




---------------

def plot_boxplot(df, feature, target, color):
    plt.figure(figsize=(10,6))
    plt.title("Boxplot of %s by Target" % feature)
    
    # For each value of the target variable, plot the boxplot of the feature
    for target_value in df[target].unique():
        sns.boxplot(x=target,
                    y=feature,
                    data=df[df[target] == target_value],
                    color=color[target_value])
        
    plt.show();




----

def plot_boxplot(df, feature, target, colors):
    plt.figure(figsize=(10,6))
    plt.title("Boxplot of %s by Target" % feature)

    # Unique values in target column
    targets = df[target].unique()

    # For each value of the target variable, plot the boxplot of the feature
    for i in range(len(targets)):
        plt.subplot(1,len(targets),i+1)
        sns.boxplot(y=df[df[target]==targets[i]][feature], color=colors[i])
        plt.title(f'Target {targets[i]}')
        
    plt.tight_layout()
    plt.show();


------------------------------------
파이썬으로 시각화하려고 하는데 어떤 개요를 가지고 시각화를 하면 좋을까?
컬럼은 이와같애
 0   대출식별번호                                   24000 non-null  int64  
 1   계약유형                                     24000 non-null  object 
 2   성별                                       16906 non-null  object 
 3   자가부동산보유여부                                14429 non-null  object 
 4   자녀수                                      24000 non-null  int64  
 5   소득금액(USD)                                24000 non-null  float64
 6   신용대출금액(USD)                              16700 non-null  float64
 7   소득유형                                     24000 non-null  object 
 8   학력                                       24000 non-null  object 
 9   결혼상태                                     24000 non-null  object 
 10  주거형태                                     24000 non-null  object 
 11  거주지역의표준화된인구                              16851 non-null  float64
 12  대출신청시점에서의일단위나이(기준시점대비상대적인시간)             19251 non-null  float64
 13  대출신청시점에서의고객의근무일수(기준시점대비상대적인시간)           16851 non-null  float64
 14  대출신청이전에고객이등록정보를변경한시점사이의일수(기준시점대비상대적인시간)  14267 non-null  float64
 15  대출신청이전에고객이신분증을변경한시점사이의일수(기준시점대비상대적인시간)   19259 non-null  float64
 16  휴대폰번호등록여부                                24000 non-null  int64  
 17  고용주연락처등록여부                               24000 non-null  int64  
 18  직장전화번호등록여부                               24000 non-null  int64  
 19  집전화번호등록여부                                24000 non-null  int64  
 20  이메일주소등록여부                                24000 non-null  int64  
 21  거주지등급                                    24000 non-null  int64  
 22  대출신청요일                                   24000 non-null  object 
 23  대출신청시간대                                  24000 non-null  int64  
 24  고객의등록지주소와실거주지일치여부                        24000 non-null  int64  
 25  고객의등록지주소와직장주소일치여부                        24000 non-null  int64  
 26  고객의실거주지와직장주소일치여부                         24000 non-null  int64  
 27  근무회사유형                                   24000 non-null  object 
 28  문서2제공여부                                  24000 non-null  int64  
 29  문서3제공여부                                  24000 non-null  int64  
 30  문서4제공여부                                  24000 non-null  int64  
 31  신용점수1                                    17854 non-null  float64
 32  신용점수2                                    15726 non-null  float64
 33  신용점수3                                    16892 non-null  float64
 34  연체여부                                     24000 non-null  int64 

여기서 결측치는 다 넣었고,

연체여부별로 범주형변수는 범주별 건수
연체여부별로 수치형변수는 도수분포표 및 정규분포표
그리는거만 했는데

추가로 할거 추천좀 해줘봐
뭘 시각화 해야할지 모르겠음





데이터를 시각화하는 것은 데이터의 특성을 이해하고, 변수 간의 관계를 파악하고, 잠재적인 패턴을 찾아내는 데 매우 유용합니다. 주어진 데이터셋과 연체여부(target variable)에 대해 이미 범주형 변수의 범주별 건수와 수치형 변수의 도수분포표 및 정규분포표를 그렸다면, 추가로 아래와 같은 시각화를 고려해볼 수 있습니다:
상관관계 행렬 (Correlation Matrix): 모든 수치형 변수 간의 상관계수를 계산하고 이를 히트맵으로 나타내 보세요. 이는 서로 다른 특성들이 어떻게 관련되어 있는지 파악하는 데 도움이 됩니다.
python
corr_matrix = df.corr()
sns.heatmap(corr_matrix)
산점도 (Scatter plot): 두 개의 수치형 변수 각각에 대한 데이터 포인트들을 2차원 공간에 점으로 나타내는 그래프입니다. 이는 두 변수 사이의 관계와 분포 형태 등을 파악하는 데 도움이 됩니다.
python
sns.scatterplot(x='feature1', y='feature2', hue='target', data=df)
Pairplot: 여러 개의 산점도를 한 번에 보여주는 방법입니다. 몇몇 선택된 특성들 사이에서 관계가 있는지 살펴보기 좋습니다.
python
sns.pairplot(df, vars=['feature1', 'feature2', 'feature3'], hue='target')
시간에 따른 변화: '대출신청요일'과 '대출신청시간대' 등 시간과 관련된 정보가 있다면, 시간에 따른 연체여부나 다른 변수들의 변화 추세를 살펴보는 것도 유용할 수 있습니다.
등급별 분석: 신용점수나 거주지등급 같은 등급 정보가 있다면, 각 등급 별로 연체율 등을 비교해보세요.
Violin plot: Box plot과 Kernel Density Plot을 결합한 형태로서, 분포의 중앙값뿐만 아니라 데이터의 분포도를 함께 보여줍니다.
python
sns.violinplot(x='target', y='feature', data=df)
이 외에도, 시각화 방법은 다양하며 데이터와 분석 목적에 따라 적절한 방법을 선택하면 됩니다. 위에서 제시한 방법들이 데이터 탐색에 도움이 되길 바랍니다.

